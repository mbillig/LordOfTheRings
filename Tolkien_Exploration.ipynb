{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marie\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wiki_en_wordids.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6432703add5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# load id->word mapping (the dictionary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_en_wordids.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# load corpus iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_en_tfidf.mm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marie\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mload_from_text\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m    350\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marie\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3u\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marie\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \"\"\"\n\u001b[1;32m--> 644\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# load id->word mapping (the dictionary)\n",
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n",
    "# load corpus iterator\n",
    "mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n",
    "# extract 100 LDA topics, using 20 full passes, (batch mode) no online updates\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "VALAQUENTA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    print(i)\n",
    "    \n",
    "silSections = ['AINULINDALE', 'VALAQUENTA', 'QUENTA SILMARILLION', 'AKALLABETH', 'OF THE RINGS OF POWER']\n",
    "\n",
    "print(silSections[1])\n",
    "chaptersPerBook = {'TheHobbit':19, 'FellowshipOfTheRing':[1, 12, 10], 'TwoTowers':[11,10], 'ReturnOfTheKing':[10,9], 'Silmarillion':[24]}\n",
    "chaptersPerBook['TwoTowers'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\LOTR\\clean\\01_FellowshipOfTheRing_clean.txt\n",
      "23\n",
      "b0_c1.txt\n",
      "b1_c1.txt\n",
      "b1_c2.txt\n",
      "b1_c3.txt\n",
      "b1_c4.txt\n",
      "b1_c5.txt\n",
      "b1_c6.txt\n",
      "b1_c7.txt\n",
      "b1_c8.txt\n",
      "b1_c9.txt\n",
      "b1_c10.txt\n",
      "b1_c11.txt\n",
      "b1_c12.txt\n",
      "b2_c1.txt\n",
      "b2_c2.txt\n",
      "b2_c3.txt\n",
      "b2_c4.txt\n",
      "b2_c5.txt\n",
      "b2_c6.txt\n",
      "b2_c7.txt\n",
      "b2_c8.txt\n",
      "b2_c9.txt\n",
      "b2_c10.txt\n",
      ".\\data\\LOTR\\clean\\02_TwoTowers_clean.txt\n",
      "21\n",
      "b3_c1.txt\n",
      "b3_c2.txt\n",
      "b3_c3.txt\n",
      "b3_c4.txt\n",
      "b3_c5.txt\n",
      "b3_c6.txt\n",
      "b3_c7.txt\n",
      "b3_c8.txt\n",
      "b3_c9.txt\n",
      "b3_c10.txt\n",
      "b3_c11.txt\n",
      "b4_c1.txt\n",
      "b4_c2.txt\n",
      "b4_c3.txt\n",
      "b4_c4.txt\n",
      "b4_c5.txt\n",
      "b4_c6.txt\n",
      "b4_c7.txt\n",
      "b4_c8.txt\n",
      "b4_c9.txt\n",
      "b4_c10.txt\n",
      ".\\data\\LOTR\\clean\\03_ ReturnOfTheKing_clean.txt\n",
      "19\n",
      "b5_c1.txt\n",
      "b5_c2.txt\n",
      "b5_c3.txt\n",
      "b5_c4.txt\n",
      "b5_c5.txt\n",
      "b5_c6.txt\n",
      "b5_c7.txt\n",
      "b5_c8.txt\n",
      "b5_c9.txt\n",
      "b5_c10.txt\n",
      "b6_c1.txt\n",
      "b6_c2.txt\n",
      "b6_c3.txt\n",
      "b6_c4.txt\n",
      "b6_c5.txt\n",
      "b6_c6.txt\n",
      "b6_c7.txt\n",
      "b6_c8.txt\n",
      "b6_c9.txt\n",
      ".\\data\\LOTR\\clean\\04_TheHobbit_clean.txt\n",
      "19\n",
      "hob_c1.txt\n",
      "hob_c2.txt\n",
      "hob_c3.txt\n",
      "hob_c4.txt\n",
      "hob_c5.txt\n",
      "hob_c6.txt\n",
      "hob_c7.txt\n",
      "hob_c8.txt\n",
      "hob_c9.txt\n",
      "hob_c10.txt\n",
      "hob_c11.txt\n",
      "hob_c12.txt\n",
      "hob_c13.txt\n",
      "hob_c14.txt\n",
      "hob_c15.txt\n",
      "hob_c16.txt\n",
      "hob_c17.txt\n",
      "hob_c18.txt\n",
      "hob_c19.txt\n",
      ".\\data\\LOTR\\clean\\05_Silmarillion_clean.txt\n",
      "sil_ainulindale.txt\n",
      "sil_valaquenta.txt\n",
      "sil_quenta_c1.txt\n",
      "sil_quenta_c2.txt\n",
      "sil_quenta_c3.txt\n",
      "sil_quenta_c4.txt\n",
      "sil_quenta_c5.txt\n",
      "sil_quenta_c6.txt\n",
      "sil_quenta_c7.txt\n",
      "sil_quenta_c8.txt\n",
      "sil_quenta_c9.txt\n",
      "sil_quenta_c10.txt\n",
      "sil_quenta_c11.txt\n",
      "sil_quenta_c12.txt\n",
      "sil_quenta_c13.txt\n",
      "sil_quenta_c14.txt\n",
      "sil_quenta_c15.txt\n",
      "sil_quenta_c16.txt\n",
      "sil_quenta_c17.txt\n",
      "sil_quenta_c18.txt\n",
      "sil_quenta_c19.txt\n",
      "sil_quenta_c20.txt\n",
      "sil_quenta_c21.txt\n",
      "sil_quenta_c22.txt\n",
      "sil_quenta_c23.txt\n",
      "sil_quenta_c24.txt\n",
      "sil_akallabeth.txt\n",
      "sil_of the rings of power.txt\n"
     ]
    }
   ],
   "source": [
    "pathToData = '.\\data\\LOTR\\clean\\\\'\n",
    "fileNames = (file for file in os.listdir(pathToData) if os.path.isfile(os.path.join(pathToData, file)))\n",
    "writePath = pathToData + '\\chapters\\\\'\n",
    "\n",
    "silSections = ['AINULINDALE', 'VALAQUENTA', 'QUENTA SILMARILLION', 'AKALLABETH', 'OF THE RINGS OF POWER']\n",
    "#chaptersPerBook = {'TheHobbit':19, 'FellowshipOfTheRing':[12, 10], 'TwoTowers':[11,10], 'ReturnOfTheKing':[10,9], 'Silmarillion':[24]}\n",
    "bookNum = 0\n",
    "chapterCutoffs = [1, 12, 10, 11, 10, 10, 9, 19]\n",
    "\n",
    "for filename in fileNames:\n",
    "    print(pathToData + filename)\n",
    "    \n",
    "    with open (pathToData + filename, 'rt', encoding=\"utf8\") as in_file: \n",
    "        chapterNum = 1\n",
    "        contents = in_file.read()\n",
    "        bookLen = len(contents)\n",
    "        bookName = filename.split('_')[1]\n",
    "        \n",
    "        if bookName != 'Silmarillion':\n",
    "            chapters = contents.split('Chapter')\n",
    "            print(len(chapters))\n",
    "            for ch in chapters:\n",
    "                newFileName = 'b' + str(bookNum) + \"_c\" + str(chapterNum) + '.txt'\n",
    "                if bookNum == 7:\n",
    "                    newFileName = \"hob_c\" + str(chapterNum) + '.txt'\n",
    "                with open(writePath + newFileName, \"w\", encoding=\"utf8\") as text_file:\n",
    "                    text_file.write(ch)\n",
    "                print(newFileName)\n",
    "                if chapterNum == chapterCutoffs[bookNum]:\n",
    "                    chapterNum = 1\n",
    "                    bookNum += 1\n",
    "                else:\n",
    "                    chapterNum += 1\n",
    "                \n",
    "        else:\n",
    "            remaining = contents\n",
    "            for iSection in range(1, len(silSections)):\n",
    "                title = silSections[iSection - 1]\n",
    "                splitSection = remaining.split(silSections[iSection], 1)\n",
    "                section = splitSection[0]\n",
    "                remaining = splitSection[1]\n",
    "                if title == 'QUENTA SILMARILLION':\n",
    "                    chapterNum = 1\n",
    "                    chapters = section.split('Chapter')\n",
    "                    for ch in chapters:\n",
    "                        newFileName = 'sil_quenta_c' + str(chapterNum) + '.txt'\n",
    "                        with open(writePath + newFileName, \"w\", encoding=\"utf8\") as text_file:\n",
    "                            text_file.write(ch)\n",
    "                        print(newFileName)\n",
    "                        chapterNum += 1\n",
    "                else:\n",
    "                    newFileName = 'sil_' + title.lower().replace(' ', '') + '.txt'\n",
    "                    with open(writePath + newFileName, \"w\", encoding=\"utf8\") as text_file:\n",
    "                        text_file.write(section)\n",
    "                    print(newFileName)\n",
    "            newFileName = 'sil_' + silSections[-1].lower() + '.txt'\n",
    "            with open(writePath + newFileName, \"w\", encoding=\"utf8\") as text_file:\n",
    "                text_file.write(remaining)\n",
    "            print(newFileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
